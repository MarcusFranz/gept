# Data Collection Services Analysis

**Analysis Date**: 2026-01-09
**Server**: Ampere A1 (Oracle Cloud) - ubuntu@150.136.170.128
**System Resources**: 24GB RAM, 4 ARM cores, 98GB data disk (85% used)

## Executive Summary

The Ampere server currently runs **7 primary data collection services** that gather OSRS Grand Exchange market data from the Wiki API. These services are running efficiently with minimal resource usage (<1% CPU, ~500MB combined memory). All services are production-ready and should be continued.

## Services Overview

### 1. **1-Minute Latest Price Collector** (Docker)
- **Container**: `osrs-latest-1m`
- **Script**: `collect_latest_1m.py`
- **Purpose**: High-frequency tick data collection (every 60 seconds)
- **Target Table**: `prices_latest_1m` (PostgreSQL)
- **API Endpoint**: `https://prices.runescape.wiki/api/v1/osrs/latest`
- **Resource Usage**:
  - CPU: 0.01%
  - Memory: 126.4 MiB
  - Metrics Port: 9103
- **Status**: ‚úÖ Healthy (Up 2 days)

### 2. **5-Minute OHLC Collector** (Docker)
- **Container**: `osrs-5m-collector`
- **Script**: `collect_5m_pg.py`
- **Purpose**: 5-minute average high/low prices with volumes
- **Target Table**: `price_data_5min` (426M rows, TimescaleDB hypertable)
- **API Endpoint**: `https://prices.runescape.wiki/api/v1/osrs/5m`
- **Interval**: 300 seconds (5 minutes)
- **Resource Usage**:
  - CPU: 0.01%
  - Memory: 100.5 MiB
  - Metrics Port: 9100
- **Status**: ‚úÖ Healthy (Up 2 days)

### 3. **Hourly Timeseries Collector** (Docker)
- **Container**: `osrs-hourly-collector`
- **Script**: `collect_hourly_pg.py`
- **Purpose**: Historical hourly OHLC data for ~65 high-volume items
- **Target Table**: `prices_1h` (PostgreSQL)
- **API Endpoint**: `https://prices.runescape.wiki/api/v1/osrs/timeseries?timestep=1h&id={item_id}`
- **Interval**: 3600 seconds (1 hour)
- **Resource Usage**:
  - CPU: 0.01%
  - Memory: 94.79 MiB
  - Metrics Port: 9101
- **Status**: ‚úÖ Healthy (Up 2 days)
- **Smart Fetching**: Only fetches items with stale data (>2 hours old)

### 4. **News Collector** (Docker)
- **Container**: `osrs-news-collector`
- **Script**: `collect_news_pg.py`
- **Purpose**: Collects OSRS official news and wiki changes from RSS feeds
- **Target Table**: `osrs_news` (PostgreSQL)
- **Sources**:
  - Official OSRS news RSS
  - Wiki recent changes RSS
- **Interval**: 1800 seconds (30 minutes)
- **Resource Usage**:
  - CPU: 0.01%
  - Memory: 39.45 MiB
  - Metrics Port: 9102
- **Status**: ‚úÖ Healthy (Up 2 days)

### 5. **Legacy 5m/Latest Collector** (Systemd)
- **Service**: Direct Python process (non-Docker)
- **Script**: `osrs_collector_pg.py`
- **Purpose**: Legacy combined 5m + latest collector
- **Status**: ‚ö†Ô∏è **REDUNDANT** - duplicates Docker services #1 and #2
- **Process ID**: 835141
- **Resource Usage**:
  - CPU: 0.2%
  - Memory: 33.5 MiB
- **Recommendation**: **DEPRECATE** - functionality covered by Docker containers

### 6. **Player Count Collector** (Systemd)
- **Service**: `item_updater.service` (running `collect_player_counts.py`)
- **Script**: `collect_player_counts.py`
- **Purpose**: Scrapes OSRS homepage for current player count
- **Target**: DuckDB file (`data/player_counts.duckdb`)
- **Interval**: 60 seconds
- **Resource Usage**:
  - CPU: 0.1%
  - Memory: 81.3 MiB
- **Status**: ‚úÖ Running (Up 1 week)
- **Note**: Uses DuckDB instead of PostgreSQL

### 7. **Item Metadata Updater** (Systemd)
- **Service**: `item_updater.service`
- **Script**: `update_items.py`
- **Purpose**: Updates item metadata (names, GE limits, values, icons)
- **API Endpoint**: `https://prices.runescape.wiki/api/v1/osrs/mapping`
- **Target**: DuckDB file (`data/items.duckdb`)
- **Interval**: 86400 seconds (24 hours)
- **Resource Usage**:
  - CPU: 0.0%
  - Memory: 73.4 MiB
- **Status**: ‚úÖ Running (Up 1 week)
- **Note**: Uses DuckDB instead of PostgreSQL

### 8. **Dashboard** (Docker)
- **Container**: `osrs-dashboard`
- **Script**: `dashboard.py`
- **Purpose**: Web-based monitoring dashboard
- **Port**: 8080
- **Resource Usage**:
  - CPU: 0.01%
  - Memory: 15.78 MiB
- **Status**: ‚ö†Ô∏è Unhealthy (but functional)
- **Features**:
  - Real-time table stats (row counts, freshness, size)
  - Container health monitoring
  - Auto-refresh every 5 seconds

## Additional Services

### 9. **GePT Discord Bot** (Systemd)
- **Service**: `gept-bot.service`
- **Type**: Node.js application
- **Purpose**: Discord bot for OSRS GE flipping recommendations
- **Working Directory**: `/home/ubuntu/gept/`
- **Resource Usage**:
  - CPU: 0.2%
  - Memory: 42.6 MiB
- **Status**: ‚úÖ Running
- **Note**: This is part of the recommendation engine, not data collection

### 10. **GePT Inference Engine** (Cron)
- **Cron Job**: `*/5 * * * * /home/ubuntu/gept/scripts/run_inference_cron.sh`
- **Purpose**: Runs ML model inference every 5 minutes
- **Working Directory**: `/home/ubuntu/gept/`
- **Status**: ‚úÖ Running via cron
- **Note**: This is the production ML prediction service

## Resource Analysis

### CPU Usage
- **Total Data Collection CPU**: <0.5% combined
- **Disk I/O**: Minimal (sdb showing 0.6% utilization during collection)
- **System Load**: 0.10, 0.39, 0.47 (15-min avg)
- **Assessment**: ‚úÖ Excellent - services are extremely efficient

### Memory Usage
- **Total System**: 24GB
- **Used**: 2.3GB
- **Available**: 15.2GB
- **Data Collection Services**: ~500MB total
- **PostgreSQL**: 4.6GB (largest consumer)
- **Assessment**: ‚úÖ Excellent - plenty of headroom

### Disk Usage
- **Data Disk** (/dev/sdb - 98GB): 79GB used (85%)
- **Primary Disk** (/dev/sda - 45GB): 23GB used (50%)
- **DuckDB Files**: 4.1GB in `/home/ubuntu/osrs_collector/data/`
- **Assessment**: ‚ö†Ô∏è Data disk approaching capacity

### Database Storage
- **PostgreSQL Database**: `osrs_data`
- **Key Tables**:
  - `price_data_5min`: 426M rows (primary data store)
  - `predictions`: Updated every 5 minutes by inference engine
  - `prices_1h`: Historical hourly data
  - `prices_latest_1m`: High-frequency tick data
  - `osrs_news`: News and wiki changes
  - `items`: 4,500 item metadata (managed elsewhere)

## Service Architecture

### Docker Stack
All collectors run in Docker containers orchestrated by `docker-compose.yml`:
- **Base Image**: Custom Python 3.10 image
- **Network**: `osrs-collectors` (isolated)
- **Volumes**: Shared data directory
- **Health Checks**: HTTP metrics endpoints
- **Logging**: JSON file driver (10MB max, 3 files)
- **Restart Policy**: `unless-stopped`

### Monitoring
- **Prometheus Metrics**: Each service exposes metrics on dedicated ports (9100-9103)
- **Metrics Tracked**:
  - Request counts (success/error)
  - Items collected counters
  - Collection duration histograms
  - Last collection timestamp gauges
- **Dashboard**: Web UI at port 8080

## Evaluation & Recommendations

### Services to Continue ‚úÖ

1. **5-Minute Collector** - Critical for ML model training data
2. **1-Minute Latest Collector** - Provides high-frequency tick data
3. **Hourly Collector** - Essential for historical analysis
4. **News Collector** - Valuable for market event correlation
5. **Player Count Collector** - Useful market activity indicator
6. **Item Metadata Updater** - Required for item reference data
7. **Dashboard** - Essential operational visibility

### Services to Deprecate ‚ùå

1. **Legacy osrs_collector_pg.py** - Redundant with Docker services #1 and #2
   - Action: Stop systemd service, remove from startup
   - Migration: Already replaced by Docker containers

### Optimization Opportunities üîß

#### 1. Consolidate Storage Backend
**Current State**: Mixed PostgreSQL + DuckDB
**Issue**: Player count and item metadata use DuckDB while price data uses PostgreSQL

**Recommendation**:
- Migrate player count collector to PostgreSQL table `player_counts`
- Migrate item metadata to PostgreSQL table `items` (already exists!)
- Benefits:
  - Unified querying across all data
  - Better integration with ML pipeline
  - Simplified backup/replication
  - Remove DuckDB dependency

#### 2. Disk Space Management
**Current State**: Data disk at 85% capacity (79GB/98GB used)

**Recommendations**:
- Implement TimescaleDB retention policies:
  - Keep 1-minute data for 7 days
  - Keep 5-minute data for 90 days
  - Keep hourly data forever (compressed)
- Enable TimescaleDB compression for old data (can reduce by 90%)
- Archive old data to cloud storage (GCS)
- Estimated savings: 20-30GB

#### 3. Docker Dashboard Health Check
**Current State**: Dashboard container shows "unhealthy"

**Action**: Fix health check in docker-compose.yml or adjust endpoint

#### 4. Code Consolidation
**Current State**: Collectors scattered across multiple directories

**Recommendation**:
- Move all collector scripts to unified `collectors/` directory
- Add to Git repository for version control
- Create proper Python package with shared utilities
- Benefits:
  - Better code reuse (DB connections, metrics, etc.)
  - Easier testing and deployment
  - Version control and change tracking

#### 5. Configuration Management
**Current State**: Hardcoded credentials in scripts

**Recommendation**:
- Move all config to environment variables (already partially done)
- Use .env file or secrets management
- Benefits:
  - Security improvement
  - Easier deployment to different environments

#### 6. Monitoring Enhancements
**Current State**: Prometheus metrics exposed but not centrally collected

**Recommendation**:
- Set up Prometheus server to scrape all metrics
- Add Grafana dashboards for visualization
- Set up alerting for:
  - Stale data (no updates in >15 minutes)
  - Collection failures
  - Disk space warnings
  - Database connection issues

## Migration Plan to Git

### Directory Structure
```
collectors/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ collectors/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ collect_5m.py
‚îÇ   ‚îú‚îÄ‚îÄ collect_1m_latest.py
‚îÇ   ‚îú‚îÄ‚îÄ collect_hourly.py
‚îÇ   ‚îú‚îÄ‚îÄ collect_news.py
‚îÇ   ‚îú‚îÄ‚îÄ collect_player_counts.py
‚îÇ   ‚îú‚îÄ‚îÄ update_items.py
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py
‚îÇ   ‚îî‚îÄ‚îÄ shared/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ db.py (common DB utilities)
‚îÇ       ‚îú‚îÄ‚îÄ metrics.py (common Prometheus setup)
‚îÇ       ‚îî‚îÄ‚îÄ config.py (configuration management)
‚îú‚îÄ‚îÄ systemd/
‚îÇ   ‚îú‚îÄ‚îÄ gept-bot.service
‚îÇ   ‚îî‚îÄ‚îÄ item_updater.service
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ DEPLOYMENT.md
```

### Migration Steps
1. ‚úÖ Analyze existing services (COMPLETED)
2. ‚è≥ Copy all scripts to local Git repository
3. ‚è≥ Refactor common code into shared modules
4. ‚è≥ Add comprehensive README and documentation
5. ‚è≥ Create deployment scripts
6. ‚è≥ Test locally with Docker Compose
7. ‚è≥ Deploy to server (non-disruptive)
8. ‚è≥ Remove legacy scripts after verification

## Data Flow Diagram

```
OSRS Wiki API
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Collection Layer (Docker)    ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Ä¢ 1m Latest (every 60s)           ‚îÇ
‚îÇ  ‚Ä¢ 5m OHLC (every 5m)              ‚îÇ
‚îÇ  ‚Ä¢ Hourly (every 1h)               ‚îÇ
‚îÇ  ‚Ä¢ News (every 30m)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PostgreSQL/TimescaleDB            ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Ä¢ price_data_5min (426M rows)     ‚îÇ
‚îÇ  ‚Ä¢ prices_latest_1m                ‚îÇ
‚îÇ  ‚Ä¢ prices_1h                       ‚îÇ
‚îÇ  ‚Ä¢ osrs_news                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ML Pipeline (every 5m)            ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Ä¢ Feature Engine (102 features)   ‚îÇ
‚îÇ  ‚Ä¢ CatBoost Model Inference        ‚îÇ
‚îÇ  ‚Ä¢ Predictions Table               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Application Layer                 ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Ä¢ Discord Bot (recommendations)   ‚îÇ
‚îÇ  ‚Ä¢ API (uvicorn on 8000)           ‚îÇ
‚îÇ  ‚Ä¢ Dashboard (port 8080)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Critical Dependencies

### External APIs
1. **OSRS Wiki API** (prices.runescape.wiki)
   - Rate limits: None enforced (but use User-Agent)
   - Reliability: Very high (>99.9% uptime)
   - Data freshness: 5-minute updates

2. **OSRS Homepage** (oldschool.runescape.com)
   - Used for: Player count scraping
   - Reliability: High
   - Fragility: HTML parsing could break with site updates

### Database
- **PostgreSQL 14** with TimescaleDB extension
- **Critical**: All services depend on DB availability
- **Connection**: localhost:5432
- **Credentials**: Hardcoded (should be in secrets)

### Infrastructure
- **Docker**: Required for collector containers
- **Systemd**: Used for non-Docker services
- **Cron**: Used for inference engine scheduling

## Security Considerations

1. **Database Credentials**: Currently hardcoded in scripts
   - Risk: Medium (scripts only readable by ubuntu/opc users)
   - Recommendation: Move to secrets management

2. **API Keys**: None required (public APIs)

3. **Network Exposure**:
   - Dashboard: Port 8080 (should be firewalled or proxied)
   - Metrics: Ports 9100-9103 (should be firewalled)
   - API: Port 8000 (currently exposed)

4. **Docker Security**:
   - Containers run as non-root
   - No privileged mode
   - Limited network access

## Testing & Validation

### Health Checks
- All Docker containers have HTTP health checks
- Dashboard provides unified health view
- Prometheus metrics track collection success/failure

### Data Quality
- Timestamp validation (avoid duplicates)
- NULL handling for missing prices
- Volume validation (reject obviously wrong data)

### Monitoring Gaps
- ‚ùå No alerting on collection failures
- ‚ùå No automated data quality checks
- ‚ùå No performance regression detection

## Conclusion

The data collection infrastructure is **well-designed, efficient, and production-ready**. All services should continue running with the following priorities:

**Immediate Actions**:
1. Stop legacy `osrs_collector_pg.py` service (redundant)
2. Fix dashboard health check
3. Migrate code to Git repository

**Short-term Improvements** (1-2 weeks):
1. Consolidate DuckDB services to PostgreSQL
2. Implement disk space management (compression/retention)
3. Set up centralized monitoring (Prometheus + Grafana)
4. Refactor shared code into modules

**Long-term Enhancements** (1-2 months):
1. Implement alerting system
2. Add data quality validation pipeline
3. Set up automated backups to cloud storage
4. Create CI/CD pipeline for collector deployments

**Resource Forecast**:
- CPU: Will remain <1% (highly efficient)
- Memory: Current usage sustainable (500MB/24GB)
- Disk: Will need compression or retention policies within 2-3 months
- Network: Minimal (~1MB/min from APIs)

The services are production-grade and require minimal maintenance. The main focus should be on consolidation, monitoring, and long-term data management rather than performance optimization.
