# Full-Scale Optuna Config - Informed by fast sweep results
# Fast sweep insights: d_model=512, layers=4, batch=512, embed=64 performed best
# This config narrows the search space to the promising region

data:
  recent_hours: 24        # 5-min resolution, 288 timesteps
  medium_days: 7          # 1-hour resolution, 168 timesteps
  long_days: 30           # 4-hour resolution, 180 timesteps
  max_gap_pct: 0.05       # Max 5% missing data
  outlier_threshold: 0.5  # Flag >50% price changes

model:
  # Multi-resolution input dimensions
  recent_len: 288         # 24h at 5-min
  recent_features: 6
  medium_len: 168         # 7d at 1-hour
  medium_features: 10
  long_len: 180           # 30d at 4-hour
  long_features: 10

  # Transformer architecture (narrowed from fast sweep)
  patch_size: 16
  d_model: 512            # Fast sweep winner, will tune [384, 512]
  n_heads: 8              # Will be tuned: [4, 8]
  n_layers: 4             # Fast sweep winner, will tune [4, 5, 6]
  d_ff: 1024              # Scale with d_model (2x)
  dropout: 0.15           # Fast sweep showed 0.15 optimal

  # Quantile prediction
  n_horizons: 7           # 1h, 2h, 4h, 8h, 12h, 24h, 48h
  n_quantiles: 5          # p10, p30, p50, p70, p90
  quantiles: [0.1, 0.3, 0.5, 0.7, 0.9]

  # Item embeddings
  n_items: 1200           # Max item_id is 1149, need headroom
  item_embed_dim: 64      # Fast sweep showed 64 > 32 > 16

  # Volume head (ENABLED)
  enable_volume_head: true
  volume_quantiles: [0.1, 0.5, 0.9]
  volume_hidden_dim: 64

training:
  batch_size: 512         # Fast sweep winner
  epochs: 30              # Full training
  max_epochs: 25          # Optuna trials - allow early stopping
  patience: 5             # More patience for full-scale
  lr: 0.0003              # Fast sweep winner, will tune around this
  warmup_steps: 500       # Full warmup
  weight_decay: 0.01      # Will be tuned
  num_workers: 4
  checkpoint_every: 5

# FULL-SCALE Optuna settings
optuna:
  n_trials_per_gpu: 3     # 8 GPUs x 3 = 24 total trials
  study_name: patchtst_volume_v1_full
  volume_weight: 0.2      # Same as fast sweep
  calibration_trials: 8   # First 8 trials before volume pruning

  # NARROWED search space (informed by fast sweep)
  # Fast sweep showed: d_model=512 > 384 > 256
  d_model_options: [384, 512]

  # Fast sweep showed: layers=4 best, 6 second
  n_layers_range: [4, 6]

  # Fast sweep showed: batch=512 best, but 768/1024 close
  batch_sizes: [384, 512, 768]

  # Fast sweep showed: embed=64 > 32 > 16
  item_embed_options: [32, 64]

  # Fast sweep showed: lr=0.0003 best, higher LR generally better
  lr_range: [0.0001, 0.0005]

  # Fast sweep showed: dropout=0.15 consistently best
  dropout_range: [0.1, 0.2]

  # Full-scale settings
  data_fraction: 1.0      # Use 100% of training data
  sweep_type: full        # Tag for traceability

output_dir: /workspace/gept/data
model_dir: /workspace/gept/models
log_dir: /workspace/logs/full
